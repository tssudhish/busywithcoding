{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized GAME()\n",
      "Ready to Train!\n"
     ]
    }
   ],
   "source": [
    "import game_2048\n",
    "import pygame\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "#import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "env =game_2048.GAME()\n",
    "\n",
    "tf.reset_default_graph() # what does it really do?\n",
    "#These lines establish the feed-forward part of the network used to choose actions\n",
    "inputs1 = tf.placeholder(shape=[1,16],dtype=tf.float32) \n",
    "W = tf.Variable(tf.random_uniform([16,4],0,0.01))\n",
    "\n",
    "Qout = tf.matmul(inputs1,W)\n",
    "\n",
    "predict = tf.argmax(Qout,1)\n",
    "\n",
    "#Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "nextQ = tf.placeholder(shape=[16,4],dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updateModel = trainer.minimize(loss)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "#saver = tf.train.Saver()\n",
    "\n",
    "s = env.reset()\n",
    "s # state at the begining - since this game has a number the state it is 0\n",
    "#0, Left,1 Down, 2 Right, 3 Up\n",
    "\n",
    "print(\"Ready to Train!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_chart(y,n,plt):\n",
    "    plt.figure()  # initialize plot\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,n])\n",
    "    axes.set_ylim([0,2048])\n",
    "    plt.yticks(np.arange(0, 2048+128, 128))\n",
    "    plt.xticks(np.arange(0, n+100, n/10))\n",
    "    plt.ylabel('Game Score')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.suptitle('Learning Progression')\n",
    "    plt.plot(y)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------allQ-------------\n",
      "[[0.03033846 0.01675909 0.01457783 0.01803046]]\n",
      "--------------Q1-------------\n",
      "[[0.03165721 0.03586709 0.02432026 0.04699398]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (1, 4) for Tensor 'Placeholder_1:0', which has shape '(16, 4)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-4eac925aef15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mtargetQ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmaxQ1\u001b[0m \u001b[1;31m# equation 1. storing long-term target memory for all the actons done till now.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[1;31m#Train our network using target and predicted Q values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mupdateModel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0minputs1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstate1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnextQ\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtargetQ\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# get the weights ignoring other outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m             \u001b[0mrewardAll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewardAll\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# total long-term reward. this means we will never reach zero.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;31m#print(\"Episode {}, reward value rAll = {}\\nIs Game dead? {}\".format(episode,rAll,d))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 958\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    959\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1155\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m                 \u001b[1;34m'which has shape %r'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1157\u001b[1;33m                 (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m   1158\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (1, 4) for Tensor 'Placeholder_1:0', which has shape '(16, 4)'"
     ]
    }
   ],
   "source": [
    "# Set learning parameters\n",
    "gamma = .99 # Gamma almost 1.0 - what does this mean? [0,1]\n",
    "# higher gammma indicates the decision which gives the best reward is given bias (0.99 vs 0.99^2 (for next move and so on))\n",
    "epsilon = 0.2   # epsilon value - How many times you want to use the predicted (learning NN) than a random input. 1 will mean never use a predicted value. 0 will mean always use the predicted value\n",
    "# epsilon - selection of random.\n",
    "num_episodes = 50#000 # a large number to stop runaway session (game)\n",
    "#create lists to contain total rewards and steps per episode\n",
    "envStepList = []  # steps in a particular game. i.e. while the game is running how many steps it went through.\n",
    "rewardList = []  # reward list - keeping record of rewards on each episode \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # call the init function to initialize tensorflow variables.\n",
    "    for episode in range(num_episodes):\n",
    "        #save_path=saver.save(sess,os.path.join(os.getcwd(),'model','model_17052020.ckpt'))\n",
    "        #Reset environment and get first new observation\n",
    "        state = env.reset() # start a new game (game.refresh()) s should return state number integer e.g. 0\n",
    "        rewardAll = 0        # reward All?\n",
    "        dead = False       # dead is False when it is running\n",
    "        envStep = 0          # step counter\n",
    "        #The Q-Network\n",
    "        while env.running:\n",
    "#             pygame.event.pump()\n",
    "            env.check_game_status() # returns True if game is win\n",
    "#             env.score_board.run_number=i\n",
    "            envStep+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            action,allQ = sess.run([predict,Qout],feed_dict={inputs1:state.reshape(1,16)})\n",
    "            print(\"--------------allQ-------------\")\n",
    "            print(allQ)\n",
    "            if np.random.rand(1) < epsilon:\n",
    "                action[0] = env.action_space_sample() # random movement from the game action space based on epsilon value\n",
    "            #Get new state and reward from environment\n",
    "            # env.step returns new state of the game, reward value (since minimization problem it could be 1/score)\n",
    "            state1,reward,dead,_ = env.step(action[0]) # same as game.move(a) where a should be  randrange(4)\n",
    "#             pygame.display.update()\n",
    "            # capture image of the step\n",
    "#             pygame.image.save(env.screen, os.path.join(os.getcwd(),'nn','episode_{:04d}_iter_{:04d}.jpg'.format(i,j)))\n",
    "            \n",
    "            #Obtain the Q' values by feeding the new state through our network\n",
    "            Q1 = sess.run(Qout,feed_dict={inputs1:state1.reshape(1,16)})# get the next Q table Q(s1,a)\n",
    "            print(\"--------------Q1-------------\")\n",
    "            print(Q1)\n",
    "            #Obtain maxQ' and set our target value for chosen action.\n",
    "            maxQ1 = np.max(Q1) # get the max Q-table value. for this random action \n",
    "            targetQ = allQ # target Q table - is zero at the begining of the run.\n",
    "            targetQ[0,action[0]] = reward + gamma*maxQ1 # equation 1. storing long-term target memory for all the actons done till now.\n",
    "            #Train our network using target and predicted Q values\n",
    "            _,W1 = sess.run([updateModel,W],feed_dict={inputs1:state1.reshape(1,16),nextQ:targetQ}) # get the weights ignoring other outputs.\n",
    "            rewardAll = max(rewardAll,reward) # total long-term reward. this means we will never reach zero.\n",
    "            #print(\"Episode {}, reward value rAll = {}\\nIs Game dead? {}\".format(episode,rAll,d))\n",
    "            state = state1 \n",
    "            if dead == True:\n",
    "                #print(env.game_state)\n",
    "                #print(\"Episode {} died after {} steps\".format(episode,envStep))\n",
    "                #Reduce chance of random action as we train the model.\n",
    "                epsilon = 1./((episode/50) + 10)\n",
    "                break\n",
    "            \n",
    "        #print(\"{}\\nEpisode {}\\n Number of Moves = {}\\n Maximum reward achieved={}\".format(\"-\"*50,i,j,rAll))\n",
    "        envStepList.append(envStep)\n",
    "        rewardList.append(rewardAll)\n",
    "        if episode%100==0 or episode==num_episodes:\n",
    "            plot_chart(rewardList,num_episodes,plt)\n",
    "print(\"Maximum Score ever achieved: {}\".format(max(rewardList)))\n",
    "\n",
    "plt.plot(rewardList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(jList,rList,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
