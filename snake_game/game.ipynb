{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Markov Processes\n",
    "\n",
    "A **Markov Process** is a tuple $ \\langle \\mathcal{S}, \\mathcal{P} \\rangle $ where $\\mathcal{S}$ is a set of **states** called the **observation space** or **state space** that the agent can be in, and $ \\mathcal{P} : \\mathcal{S}^2 \\to \\left[ 0, 1 \\right]$\\* is a function describing the probability of transitioning from one state to another.\n",
    "$$\n",
    "\\mathcal{P}(s_t, s_{t+1}) = \\mathbb{P} \\left[s_{t+1} \\vert s_t \\right]\n",
    "$$\n",
    "A Markov Processes are used to model stochastic sequences of states $s_0, s_1, \\dots$ satisfying the **Markov property**:\n",
    "$$\n",
    "\\mathbb{P} \\left[ s_{t+1} \\vert s_t \\right] = \\mathbb{P} \\left[ s_{t+1} \\vert s_0, s_1, \\dots, s_t \\right]\n",
    "$$\n",
    "that is, the probability of transitioning from state $s_t$ to state $s_{t+1}$ is independent of previous transitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\*$ \\mathcal{P} : \\mathcal{S}^2 \\to \\left[ 0, 1 \\right]$ : probability $\\mathcal{P}$ for state $\\mathcal{S}$ is a real number between $0$ to $1$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_state_transition_matrix(num_states):\n",
    "    P = np.random.rand(num_states, num_states)\n",
    "#     print(\"Random probability at start: {}\".format(P))\n",
    "    for i in range(num_states): # convert each row to a probability distribution by dividing by the total\n",
    "#         print(\"for state {}: Probability at start: {}\".format(i,P[i]))\n",
    "        P[i] /= sum(P[i])\n",
    "#         print(\"for state {}: Probability calculated: {}\".format(i,P[i]))\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: \n",
      "[[0.25320942 0.65117773 0.04990971 0.04570314]\n",
      " [0.03025209 0.11811782 0.39665522 0.45497487]\n",
      " [0.30548092 0.17494045 0.41678955 0.10278908]\n",
      " [0.33434821 0.30711475 0.17095575 0.18758129]]\n"
     ]
    }
   ],
   "source": [
    "num_states = 4\n",
    "P = generate_state_transition_matrix(num_states)\n",
    "print(f'P: \\n{P}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trajectories\n",
    "\n",
    "A **trajectory** (denoted $\\tau$) for a Markov process is a (potentially infinite) sequence of states\n",
    "$$\n",
    "\\tau = \\langle s_0, s_1, \\dots, s_T \\rangle\n",
    "$$\n",
    "The probability of generating particular trajectories depends on the underlying dynamics of the process, which is determined by $\\mathcal{P}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(P, T=100, s_0 = 0):\n",
    "    num_states = len(P)\n",
    "    s_t = s_0\n",
    "    tau = np.zeros(T, dtype=np.int32)\n",
    "    tau[0] = s_t\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        # Choose the next state using P[s_t] as the state \n",
    "        # transition probability distribution.\n",
    "        s_t = np.random.choice(np.arange(num_states), p=P[s_t])\n",
    "        tau[t] = s_t\n",
    "        \n",
    "    return tau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25320942 0.65117773 0.04990971 0.04570314]\n",
      " [0.03025209 0.11811782 0.39665522 0.45497487]\n",
      " [0.30548092 0.17494045 0.41678955 0.10278908]\n",
      " [0.33434821 0.30711475 0.17095575 0.18758129]]\n",
      "tau: \n",
      "[0 1 2 1 1 2 0 1 3 3 1 2 2 0 1 2 0 1 2 2 2 1 3 2 1 2 0 1 2 2 2 1 1 3 3 1 1\n",
      " 3 0 1 2 0 0 2 2 0 1 2 2 2 2 0 1 3 3 3 3 1 3 3 1 3 1 3 0 1 2 0 1 2 2 2 2 0\n",
      " 0 0 1 3 1 2 2 2 1 2 0 0 1 2 2 1 2 2 0 2 2 2 0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "T = 100\n",
    "s_0 = 0\n",
    "print(P)\n",
    "tau = generate_trajectory(P, T, s_0)\n",
    "print(f'tau: \\n{tau}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Markov Reward Processes\n",
    "\n",
    "A **Markov Reward Process**  is an extension of a Markov Process that allows us to associate rewards with states. Formally, it is a tuple $\\langle \\mathcal{S}, \\mathcal{P}, \\mathcal{R} \\rangle$ that allows us to associate with each state transition $\\langle s_t, s_{t+1} \\rangle$ some reward\n",
    "$$\n",
    "\\mathcal{R}(s_t, s_{t+1}) = \\mathbb{E}\\left[r_t \\vert s_t, s_{t+1} \\right]\n",
    "$$\n",
    "which is often simplified to being $\\mathcal{R}(s_t)$, the reward of being in a particular state $s_t$. For the purpose of this lesson and essentially all implementations, we make this simplification. The reward $r_t$ can be thought of as a measure of how good a certain state $s_t$ is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`def generate_reward_matrix(num_states, mu=0, sigma=1)` is a random reward generator. This should be replaced by the reward generated by playing the env (Game) by the agent (AI). Here for each of the four states the reward for that state is randomly calculated. <br>\n",
    "`mu`: $\\mu$ constant value <br>\n",
    "`sigma`: $\\sigma$ randomness bias<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reward_matrix(num_states, mu=0, sigma=1):\n",
    "    return mu + np.random.randn(num_states)*sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: \n",
      "[[0.28351606 0.27946024 0.19412972 0.24289398]\n",
      " [0.36162044 0.19344848 0.15779223 0.28713885]\n",
      " [0.31932415 0.3594696  0.23464985 0.08655639]\n",
      " [0.46308353 0.01223841 0.24706548 0.27761258]] \n",
      "R: \n",
      "[ 3.59570453 -6.12666783 -4.73681647 -1.04667645]\n"
     ]
    }
   ],
   "source": [
    "num_states = 4\n",
    "mu = 0\n",
    "sigma = 4\n",
    "P = generate_state_transition_matrix(num_states)\n",
    "R = generate_reward_matrix(num_states, mu, sigma)\n",
    "print(f'P: \\n{P} \\nR: \\n{R}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(P, R, T=100, s_0 = 0):\n",
    "    num_states = len(P)\n",
    "    s_t = s_0\n",
    "    tau = np.zeros(T, dtype=np.int32)\n",
    "    tau[0] = s_t\n",
    "    rewards = np.zeros(T)\n",
    "    rewards[0] = R[s_0]\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        s_t = np.random.choice(np.arange(num_states), p=P[s_t])\n",
    "        tau[t] = s_t\n",
    "        rewards[t] = R[s_t]\n",
    "        \n",
    "    return tau, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 3\trewards:-1.046676453919175\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 2\trewards:-4.736816470765706\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 1\trewards:-6.126667827952951\n",
      "\n",
      "tau: 3\trewards:-1.046676453919175\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 1\trewards:-6.126667827952951\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 2\trewards:-4.736816470765706\n",
      "\n",
      "tau: 1\trewards:-6.126667827952951\n",
      "\n",
      "tau: 1\trewards:-6.126667827952951\n",
      "\n",
      "tau: 3\trewards:-1.046676453919175\n",
      "\n",
      "tau: 3\trewards:-1.046676453919175\n",
      "\n",
      "tau: 3\trewards:-1.046676453919175\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 1\trewards:-6.126667827952951\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 2\trewards:-4.736816470765706\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 1\trewards:-6.126667827952951\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 1\trewards:-6.126667827952951\n",
      "\n",
      "tau: 1\trewards:-6.126667827952951\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 1\trewards:-6.126667827952951\n",
      "\n",
      "tau: 1\trewards:-6.126667827952951\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 3\trewards:-1.046676453919175\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 1\trewards:-6.126667827952951\n",
      "\n",
      "tau: 1\trewards:-6.126667827952951\n",
      "\n",
      "tau: 1\trewards:-6.126667827952951\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 2\trewards:-4.736816470765706\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 3\trewards:-1.046676453919175\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 3\trewards:-1.046676453919175\n",
      "\n",
      "tau: 2\trewards:-4.736816470765706\n",
      "\n",
      "tau: 3\trewards:-1.046676453919175\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 1\trewards:-6.126667827952951\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 1\trewards:-6.126667827952951\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 1\trewards:-6.126667827952951\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 2\trewards:-4.736816470765706\n",
      "\n",
      "tau: 2\trewards:-4.736816470765706\n",
      "\n",
      "tau: 2\trewards:-4.736816470765706\n",
      "\n",
      "tau: 1\trewards:-6.126667827952951\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 2\trewards:-4.736816470765706\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 3\trewards:-1.046676453919175\n",
      "\n",
      "tau: 3\trewards:-1.046676453919175\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 3\trewards:-1.046676453919175\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 1\trewards:-6.126667827952951\n",
      "\n",
      "tau: 3\trewards:-1.046676453919175\n",
      "\n",
      "tau: 3\trewards:-1.046676453919175\n",
      "\n",
      "tau: 3\trewards:-1.046676453919175\n",
      "\n",
      "tau: 3\trewards:-1.046676453919175\n",
      "\n",
      "tau: 2\trewards:-4.736816470765706\n",
      "\n",
      "tau: 2\trewards:-4.736816470765706\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 1\trewards:-6.126667827952951\n",
      "\n",
      "tau: 3\trewards:-1.046676453919175\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 2\trewards:-4.736816470765706\n",
      "\n",
      "tau: 2\trewards:-4.736816470765706\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 2\trewards:-4.736816470765706\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 3\trewards:-1.046676453919175\n",
      "\n",
      "tau: 2\trewards:-4.736816470765706\n",
      "\n",
      "tau: 2\trewards:-4.736816470765706\n",
      "\n",
      "tau: 2\trewards:-4.736816470765706\n",
      "\n",
      "tau: 1\trewards:-6.126667827952951\n",
      "\n",
      "tau: 3\trewards:-1.046676453919175\n",
      "\n",
      "tau: 2\trewards:-4.736816470765706\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 3\trewards:-1.046676453919175\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 2\trewards:-4.736816470765706\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n",
      "tau: 1\trewards:-6.126667827952951\n",
      "\n",
      "tau: 0\trewards:3.5957045347475116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tau, rewards = generate_trajectory(P, R, 100, 0)\n",
    "for trajectory, reward in zip(tau, rewards):\n",
    "    print(f'tau: {trajectory}\\trewards:{reward}\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Return and Discounted Return\n",
    "\n",
    "If we think of an agent as living in a Markov Reward Process, and we think of the reward $r_t$ as measuring the 'goodness' of certain states $s_t$, then we are interested in trajectories that **maximize** the total rewards over a trajectory (think of it as planning a trip that visits the best tourist spots). We call the cumulative rewards over a trajectory the **return** $R_t$:\n",
    "$$\n",
    "\\begin{align}\n",
    "    R_t &= r_t + r_{t+1} +  r_{t+2}+  \\dots + r_T  \\\\\n",
    "    &= \\sum_{k=t}^T r_k\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "When $T$ is finite, we say that the trajectory has a **finite time horizon** and that the environment is **episodic** (happens in episodes).\n",
    "\n",
    "In general, we say that Markov Reward Processes always have an infinite time horizon (i.e., $T = \\infty$). Episodic environments are just a special case whereby $s_{t+1} = s_t$ for all $t>T$. \n",
    "\n",
    "If we want to find trajectories that maximize the $R_t$ over an infinite time horizon, the math can get a little hairy without a guarantee that $R_t$ converges. As a result, we might consider discounting rewards exponentially over time in order to guarantee convergence. This line of reasoning leads us to the **discounted return** $G_t$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    G_t &= r_{t} + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots \\\\\n",
    "    &= \\sum_{k=t}^\\infty \\gamma^{k-t} r_k \n",
    "\\end{align}\n",
    "$$\n",
    "where $\\gamma$ is a discount factor between $0$ and $1$ (often close to $1$).\n",
    "\n",
    "We sometimes refer to both the discounted and undiscounted return as just \"return\" for brevity, and write $G_t$ where for some episodic environments it may be more appropriate to use $R_t$. In fact, it should not be hard to see that $R_t$ is just $G_t$ with $r_t = 0$ for $t > T$ and $\\gamma = 1$.\n",
    "\n",
    "***\n",
    "### Value Function\n",
    "\n",
    "We can use the expected value of $G_t$ to determine the **value** of being a certain state $s_t$:\n",
    "\n",
    "\\begin{align}\n",
    "V(s_t) &= \\mathbb{E}\\left[ G_t \\big\\vert s_t \\right] \\\\\n",
    "        &= \\mathbb{E} \\left[ r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} \\dots\\right]\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "Essentially, <u>the value tells us, given a Markov Reward Process with state transition dynamics $\\mathcal{P}$, what should I expect $G_t$ to be from **this state onwards**?</u>\n",
    "\n",
    "We can decompose $V(s_t)$ into two parts: the immediate reward $r_t$ and the discounted value of being in the next state $s_{t+1}$:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "    \\begin{split}\n",
    "        V(s_t) &= \\mathbb{E} \\left[ G_t \\big\\vert s_t \\right] \\\\\n",
    "                &= \\mathbb{E} \\left[ r_{t} + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots \\big\\vert s_t \\right] \\\\\n",
    "                &= \\mathbb{E}\\left[r_{t} + \\gamma (r_{t+1} + \\gamma r_{t+2} + \\dots) \\big\\vert s_t \\right] \\\\\n",
    "                &= \\mathbb{E}\\left[ r_{t} + \\gamma G_{t+1} \\big\\vert s_t \\right] \\\\\n",
    "                &= \\mathbb{E} \\left[ r_{t} + \\gamma V(s_{t+1}) \\big\\vert s_t \\right] \\\\\n",
    "    \\end{split}\n",
    "\\end{align}\n",
    "\n",
    "$r_t$ - immediate reward for being in the current state $s_t$<br>\n",
    "$$V(s_{t+1}) \\big\\vert s_t$$ is the value of the next state $t+1$ <br>\n",
    "This last form of $V(s_t)$ is known as the **Bellman Equation**.\n",
    "\n",
    "***\n",
    "# TD(0)\n",
    "\n",
    "Say we are interesting in learning to predict $V(s_t)$. We can use a simple method called **TD(0)** to approximate $V(s_t)$ when our observation space $\\mathcal{S}$ is **discrete** (i.e., finite). We begin by initializing a vector $V$ with zeros, which will serve as our initial predictions for what $V(s_t)$ is. Technically speaking, it can be seeded with random values and is guaranteed to converge to the right prediction given infinite data, but guessing all zeros is unbiased in terms of sign.\n",
    "\n",
    "According to the Bellman Equation, we can use \n",
    "\n",
    "$$\n",
    "r_t + \\gamma V(s_{t+1})\n",
    "$$\n",
    "\n",
    "(which we call the **TD-target**) as an unbiased estimator for $V(s_t)$. This is because we have this form of the equation:\n",
    "\n",
    "$$\n",
    "V(s_t) = \\mathbb{E} \\left[ r_{t} + \\gamma V(s_{t+1}) \\big\\vert s_t \\right]\n",
    "$$\n",
    "\n",
    "Then we can modify our estimate of $V(s_t)$ to more closely match $r_t + \\gamma V(s_{t+1})$ according to a learning rate $\\alpha$ as follows:\n",
    "\n",
    "$$\n",
    "V(s_t) \\gets V(s_t) + \\alpha \\left( r_t + \\gamma V(s_{t+1}) - V(s_t) \\right)\n",
    "$$\n",
    "\n",
    "where $r_t + \\gamma V(s_{t+1}) - V(s_t)$ is the difference between our current prediction $V(s_t)$ and the slightly better estimate: $r_t + V(s_{t+1})$. By adding this difference to our current prediction, scaled by some learning factor $\\alpha$, our estimate $V(s_t)$ slowly converges to the correct value.\n",
    "\n",
    "Let's use the TD(0) algorithm to attempt to learn the value function of this Markov reward process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_0(P, R, T=100, s_0=0, gamma=0.99, alpha=0.1):\n",
    "    num_states = len(P)\n",
    "#     print(f\"Number of states: {num_states}\")\n",
    "    V = np.zeros(num_states)\n",
    "#     print(f'Initialized value vector to zeros: {V}')\n",
    "#     print(f\"Input Reward: {R}\")\n",
    "    s_t = s_0\n",
    "    r_t = R[s_t]\n",
    "#     print(f\"Initial Reward {r_t} for initial state {s_t}\")\n",
    "    value_list=[]\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        s_t_next = np.random.choice(np.arange(num_states), p=P[s_t])\n",
    "#         print(f'Getting random state: {s_t_next}')\n",
    "        V[s_t] = V[s_t] + alpha*(r_t + gamma*V[s_t_next] - V[s_t])\n",
    "        value_list.append(V[s_t])\n",
    "        s_t = s_t_next\n",
    "        r_t = R[s_t]\n",
    "    return V,value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V: \n",
      "[-114.53398961 -121.75762328 -122.44340241 -116.5729348 ]\n"
     ]
    }
   ],
   "source": [
    "V,value_list = TD_0(P, R, 10000, 0, 0.99, 0.1)\n",
    "print(f'V: \\n{V}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(value_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Markov Decision Processes\n",
    "A **Markov Decision Process** (MDP) is an extension of a Markov Reward Process that allows state transitions to be conditional upon some action. Formally, it is a tuple $\\langle \\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R} \\rangle $ where $\\mathcal{A}$ is a set of actions available to an agent in a state $s_t$. Our reward is conditional now upon both the state $s_t$ we are in and the action $a_t$ that we took:\n",
    "\n",
    "$$\n",
    "\\mathcal{R}(s_t, a_t) = \\mathbb{E} \\left[ r_t \\vert s_t, a_t \\right]\n",
    "$$\n",
    "\n",
    "Also, $\\mathcal{P}$ is the probability of transitioning to state $s_{t+1}$ given that the current state is $s_t$ and the current action is $a_t$:\n",
    "\n",
    "$$\n",
    "\\mathcal{P}(s_t, a_t, s_{t+1}) = \\mathbb{P} \\left[ s_{t+1} \\vert s_t, a_t \\right]\n",
    "$$\n",
    "\n",
    "Whereas in an MRP the probability of generating trajectories is dependant upon only the dynamics of the underlying Markov process, in an MDP trajectories also depend on the actions of an agent.\n",
    "\n",
    "This is an MDP with four states and two actions.\n",
    "\n",
    "![**missing image (MDP)**](images/mdp.png)\n",
    "\n",
    "In this case, $\\mathcal{P}$ is a **tensor** of shape $4 \\times 2 \\times 4$ where $\\mathcal{P}(i, j, k)$ is the probability of transitioning from state $i$ to state $k$ given action $j$. Furthermore, $\\mathcal{R}$ is a matrix of shape $4 \\times 2$ where $\\mathcal{R}(i, j)$ is the reward associated with choosing action $j$ in state $i$.\n",
    "\n",
    "***\n",
    "**[Sudhish]**<br>\n",
    "<i>\n",
    "    \n",
    "<u>For Game 2048.</u><br>\n",
    "The state should be a representation of the total board. ~So it is a single entity.~<br>\n",
    "Therefore, it could be a single string of numbers or a big unique sequence of numbers. As suggested in the technical paper at location: <a href=\"\">Link to paper</a>, it could be a string representing the binary code of the board.<br>\n",
    "Action space is four since we are allowed to move in four directions <br>`[\"up\", \"down\", \"left\", \"right\"]`\n",
    "and therefore the probability **tensor** $\\mathcal{P}$ is a **tensor** of ~shape $1 \\times 4 \\times 1$~\n",
    "\n",
    "which would mean for each of the state $\\mathcal{s_t}$ on doing a particular action a probabity $\\mathcal{P}$ of getting a new state $\\mathcal{s_{t+1}}$ is a vector of size $4$\n",
    "\n",
    "</i>\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "### Policies\n",
    "\n",
    "Our goal is to define a **policy** $\\pi$, which can be thought of as a set of rules for choosing actions based on the state. Typically, we represent $\\pi$ as a probability distribution:\n",
    "$$\n",
    "\\pi: \\mathcal{S} \\times \\mathcal{A} \\to \\left[ 0, 1 \\right]\n",
    "$$\n",
    "\n",
    "where we sample $a_t$ from the distribution\n",
    "$$\n",
    "a_t \\sim \\pi(\\cdot \\vert s_t)\n",
    "$$\n",
    "For discrete action spaces and observation spaces, we can think of this as a table of size $\\lvert \\mathcal{S} \\rvert \\times \\lvert \\mathcal{A} \\rvert$ where $\\pi_{i,j}$ is the probability of choosing action $j$ in state $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_state_transition_matrix(num_states, num_actions):\n",
    "    P = np.random.rand(num_states, num_actions, num_states)\n",
    "    for i in range(num_states):\n",
    "        for j in range(num_actions):\n",
    "            P[i, j] /= sum(P[i, j])\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reward_matrix(num_states, num_actions, mu=0, sigma=1):\n",
    "    return mu + np.random.randn(num_states, num_actions)*sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_policy(num_states, num_actions):\n",
    "    pi = np.random.rand(num_states, num_actions)\n",
    "    for i in range(num_states):\n",
    "        pi[i] /= sum(pi[i])\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: \n",
      "[[[0.0631013  0.12746164 0.74031    0.06912706]\n",
      "  [0.02814893 0.35979854 0.29041578 0.32163676]]\n",
      "\n",
      " [[0.03707775 0.36471177 0.00859537 0.5896151 ]\n",
      "  [0.01284714 0.3617983  0.36131022 0.26404435]]\n",
      "\n",
      " [[0.30223627 0.25855098 0.19687173 0.24234103]\n",
      "  [0.27973137 0.22907367 0.26514599 0.22604896]]\n",
      "\n",
      " [[0.47244158 0.14255908 0.2527078  0.13229155]\n",
      "  [0.2928172  0.26456432 0.37309872 0.06951976]]] \n",
      "R: \n",
      "[[-1.33322754  0.12741028]\n",
      " [-0.04594702  1.60630708]\n",
      " [-1.49489408 -0.83986832]\n",
      " [ 0.55894308 -2.06492487]] \n",
      "pi: \n",
      "[[0.47787593 0.52212407]\n",
      " [0.36760772 0.63239228]\n",
      " [0.6950617  0.3049383 ]\n",
      " [0.84951524 0.15048476]]\n"
     ]
    }
   ],
   "source": [
    "num_states = 4\n",
    "num_actions = 2\n",
    "\n",
    "P = generate_state_transition_matrix(num_states, num_actions)\n",
    "R = generate_reward_matrix(num_states, num_actions)\n",
    "pi = generate_policy(num_states, num_actions)\n",
    "print(f'P: \\n{P} \\nR: \\n{R} \\npi: \\n{pi}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(P, R, pi, T=100, s_0 = 0):\n",
    "    num_states = len(P)\n",
    "    num_actions = len(P[0])\n",
    "    \n",
    "    s_t = s_0\n",
    "    tau = np.zeros(T, dtype=np.int32)\n",
    "    tau[0] = s_t\n",
    "    \n",
    "    a_t = np.random.choice(np.arange(num_actions), p=pi[s_t])\n",
    "    actions = np.zeros(T, dtype=np.int32)\n",
    "    actions[0] = a_t \n",
    "    \n",
    "    rewards = np.zeros(T)\n",
    "    rewards[0] = R[s_t, a_t]\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        s_t = np.random.choice(np.arange(num_states), p=P[s_t, a_t])\n",
    "        a_t = np.random.choice(np.arange(num_actions), p=pi[s_t])\n",
    "        tau[t] = s_t\n",
    "        actions[t] = a_t\n",
    "        rewards[t] = R[s_t, a_t]\n",
    "\n",
    "    return tau, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tau: \n",
      "[0 3 1 3 2 1 2 1 3 1 2 2 3 0 1 2 1 1 1 2 0 1 3 1 1 2 1 1 1 3 2 3 0 3 3 0 1\n",
      " 1 3 3 0 2 2 0 2 0 2 3 1 3 0 3 0 2 2 2 0 2 3 2 0 1 3 0 2 2 2 0 1 3 0 3 2 1\n",
      " 2 2 3 0 2 0 3 1 1 1 0 2 2 2 3 0 0 3 2 3 0 0 1 1 1 3] \n",
      "actions: \n",
      "[1 0 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1\n",
      " 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 1 0 0 1\n",
      " 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 1 1 1 0] \n",
      "rewards: \n",
      "[ 0.12741028  0.55894308 -0.04594702 -2.06492487 -0.83986832  1.60630708\n",
      " -1.49489408 -0.04594702  0.55894308  1.60630708 -0.83986832 -1.49489408\n",
      "  0.55894308  0.12741028  1.60630708 -1.49489408  1.60630708 -0.04594702\n",
      "  1.60630708 -1.49489408  0.12741028 -0.04594702  0.55894308 -0.04594702\n",
      "  1.60630708 -0.83986832  1.60630708  1.60630708 -0.04594702  0.55894308\n",
      " -0.83986832  0.55894308  0.12741028  0.55894308  0.55894308 -1.33322754\n",
      "  1.60630708  1.60630708  0.55894308  0.55894308  0.12741028 -1.49489408\n",
      " -0.83986832 -1.33322754 -1.49489408 -1.33322754 -1.49489408  0.55894308\n",
      "  1.60630708  0.55894308  0.12741028  0.55894308  0.12741028 -1.49489408\n",
      " -0.83986832 -0.83986832  0.12741028 -0.83986832  0.55894308 -1.49489408\n",
      "  0.12741028 -0.04594702  0.55894308  0.12741028 -0.83986832 -0.83986832\n",
      " -0.83986832  0.12741028 -0.04594702  0.55894308  0.12741028  0.55894308\n",
      " -1.49489408  1.60630708 -1.49489408 -1.49489408  0.55894308  0.12741028\n",
      " -1.49489408  0.12741028  0.55894308 -0.04594702  1.60630708 -0.04594702\n",
      "  0.12741028 -1.49489408 -1.49489408 -1.49489408  0.55894308 -1.33322754\n",
      "  0.12741028  0.55894308 -1.49489408  0.55894308  0.12741028  0.12741028\n",
      "  1.60630708  1.60630708  1.60630708  0.55894308]\n"
     ]
    }
   ],
   "source": [
    "tau, actions, rewards = generate_trajectory(P, R, pi)\n",
    "print(f'tau: \\n{tau} \\nactions: \\n{actions} \\nrewards: \\n{rewards}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "1. Markov Decision Processes are mathematical models of the world that consider how the world changes based on underlying state dynamics and decisions of agents. It also measures the *goodness* of certain states by associating with each state a reward.\n",
    "2. We can use the TD(0) algorithm to learn the value function $V$ for discrete state spaces."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
